{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from joblib import load\n",
    "from pydantic import BaseModel\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "entities_dict = {}\n",
    "duration_entities={}\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "query = input(\"Enter your text:\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "origins = [\n",
    "    \"http://localhost\",\n",
    "    \"http://localhost:5173\",\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    uid: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "def classifyQuery(query):\n",
    "    \n",
    "   \n",
    "    model = load('./model/model.joblib')\n",
    "    fitted_vectorizer = load('./model/fitted_vectorizer.joblib')\n",
    "    \n",
    "    typeId, = model.predict(fitted_vectorizer.transform([query]))\n",
    "    \n",
    "\n",
    "    match typeId:\n",
    "        case 0:\n",
    "            return \"Task\"\n",
    "        \n",
    "        case 1:\n",
    "            return \"Event\"\n",
    "            \n",
    "        case 2:\n",
    "            return \"Routine\"\n",
    "            \n",
    "        case 3:\n",
    "            return \"Project\"\n",
    "        case 4:\n",
    "            return \"Venture\"\n",
    "        \n",
    "def extractName():\n",
    "        \n",
    "    # Define the regular expression pattern\n",
    "        pattern = r'@\\w+(?:\\s+\\w+)*@'\n",
    "\n",
    "        # Find all matches of the regular expression in the text input\n",
    "        matches = re.findall(pattern, query)\n",
    "\n",
    "        # Loop through the matches and extract the words between the \"@\" signs\n",
    "        for match in matches:\n",
    "            # Remove the \"@\" signs and any leading or trailing whitespace\n",
    "            words = match.strip('@').strip()\n",
    "\n",
    "            # Tokenize the words using Spacy\n",
    "            words_doc = nlp(words)\n",
    "\n",
    "        \n",
    "        \n",
    "            list=[token.text for token in words_doc]\n",
    "            output_string = ' '.join(list)\n",
    "            entities_dict['name'] = output_string\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extractEntities(query,typeId):\n",
    "    \n",
    "    \n",
    "    doc = nlp(query)\n",
    "\n",
    "    start_time = None\n",
    "\n",
    "    \n",
    "    if (typeId==0) or(typeId==1) or(typeId==2):\n",
    "\n",
    "        extractName()\n",
    "\n",
    "\n",
    "        \n",
    "        for token in doc:\n",
    "            # Check if the token is a number\n",
    "            if token.pos_ == \"NUM\":\n",
    "                # Check if the next token is \"pm\" or \"am\"\n",
    "                next_token = doc[token.i+1] if token.i+1 < len(doc) else None\n",
    "                if not start_time:\n",
    "                        # This is the start time\n",
    "                        start_time = token.text + \" \" + next_token.text\n",
    "                        time_obj = datetime.strptime(start_time, '%I.%M %p')\n",
    "                        time_24_str = time_obj.strftime('%H:%M')\n",
    "                        entities_dict['s'] = time_24_str\n",
    "\n",
    "                       # print(\"Start time:\", start_time)\n",
    "                        \n",
    "                        \n",
    "                elif next_token and (next_token.text == \"hours\"):\n",
    "                    # The number is part of a duration expression\n",
    "                    duration_hours = str(token.text)\n",
    "                    entities_dict['duration']=duration_hours+\" hours\"\n",
    "                    duration_entities['h']=duration_hours\n",
    "                    duration_entities['m']=0\n",
    "                    entities_dict['duration']=duration_entities\n",
    "                    \n",
    "\n",
    "                    \n",
    "                \n",
    "                # print(\"Duration:\", duration, \"hours\")\n",
    "\n",
    "                elif next_token and (next_token.text == \"minutes\"):\n",
    "                    duration_minutes=str(token.text)\n",
    "                    duration_entities['h']=duration_hours\n",
    "                    duration_entities['m']=duration_minutes\n",
    "                    entities_dict['duration']=duration_entities\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                else:\n",
    "                    end_time=token.text+ \" \" + next_token.text\n",
    "                    entities_dict['end_time'] = end_time\n",
    "                    #print(\"End time:\", end_time)\n",
    "\n",
    "    \n",
    "        print(entities_dict)\n",
    "\n",
    "\n",
    "    if (typeId==3): \n",
    "\n",
    "        extractName()\n",
    "        # Extract date\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"DATE\":\n",
    "                date_entity=ent.text\n",
    "                #print(ent.text)\n",
    "                entities_dict['due']=date_entity\n",
    "        print(entities_dict)\n",
    "\n",
    "\n",
    "    if (typeId==4):\n",
    "        extractName()\n",
    "\n",
    "\n",
    "        print(entities_dict)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "classifyQuery(query)\n",
    "typeId=classifyQuery(query)\n",
    "extractEntities(query,typeId)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
