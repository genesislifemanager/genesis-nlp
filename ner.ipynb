{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from joblib import load\n",
    "from pydantic import BaseModel\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "entities_dict = {}\n",
    "duration_entities = {}\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extractName(query,entities_dict):\n",
    "\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'@\\w+(?:\\s+\\w+)*@'\n",
    "\n",
    "    # Find all matches of the regular expression in the text input\n",
    "    matches = re.findall(pattern, query)\n",
    "\n",
    "    # Loop through the matches and extract the words between the \"@\" signs\n",
    "    for match in matches:\n",
    "        # Remove the \"@\" signs and any leading or trailing whitespace\n",
    "        words = match.strip('@').strip()\n",
    "\n",
    "        # Tokenize the words using Spacy\n",
    "        words_doc = nlp(words)\n",
    "\n",
    "        list = [token.text for token in words_doc]\n",
    "        output_string = ' '.join(list)\n",
    "        entities_dict['name'] = output_string\n",
    "\n",
    "\n",
    "def extractEntities(query, type):\n",
    "    doc = nlp(query)\n",
    "    start_time = None\n",
    "\n",
    "    entities_dict = {}\n",
    "    duration_entities = {}\n",
    "    token_check_min=\"minutes\"\n",
    "    token_check_hour=\"hours\"\n",
    "    token1 = nlp(token_check_min)[0]\n",
    "    token2=nlp(token_check_hour)[0]\n",
    "\n",
    "    if (type == \"Task\") or (type == \"Event\") or (type == \"Routine\"):\n",
    "\n",
    "        extractName(query,entities_dict)\n",
    "        duration_hours=0\n",
    "        duration_minutes=0\n",
    "\n",
    "        for token in doc:\n",
    "            # Check if the token is a number\n",
    "            if token.pos_ == \"NUM\":\n",
    "                # Check if the next token is \"pm\" or \"am\"\n",
    "                next_token = doc[token.i+1] if token.i+1 < len(doc) else None\n",
    "                if next_token and (next_token.text == \"pm\" or next_token.text == \"am\" ):\n",
    "                    # This is the start time\n",
    "                    start_time = token.text + \" \" + next_token.text\n",
    "                    time_obj = datetime.strptime(start_time, '%I.%M %p')\n",
    "                    time_24_str = time_obj.strftime('%H:%M')\n",
    "                    entities_dict['s'] = time_24_str\n",
    "\n",
    "                   # print(\"Start time:\", start_time)\n",
    "\n",
    "                if next_token and (next_token.text == \"hours\"):\n",
    "                        duration_hours = str(token.text)\n",
    "                        #print(duration_hours)\n",
    "                        if token1 in doc:\n",
    "                                for token in doc:\n",
    "                                        if next_token and (next_token.text == \"minutes\"):\n",
    "                                                duration_minutes=str(token.text)\n",
    "                                        else:\n",
    "                                            duration_minutes=0\n",
    "                       \n",
    "\n",
    "                \n",
    "\n",
    "                if next_token and (next_token.text == \"minutes\"):\n",
    "                       duration_minutes=str(token.text)\n",
    "                       if token2 in doc:\n",
    "                                for token in doc:\n",
    "                                        if next_token and (next_token.text == \"hours\"):\n",
    "                                                duration_hours=str(token.text)\n",
    "                                        else:\n",
    "                                            duration_hours=0\n",
    "\n",
    "\n",
    "        duration_entities['h']=duration_hours\n",
    "        duration_entities['m']=duration_minutes\n",
    "        entities_dict['duration']=duration_entities\n",
    "                                       \n",
    "\n",
    "    ''''elif next_token and (next_token.text == \"hours\"):\n",
    "                    # The number is part of a duration expression\n",
    "                    duration_hours = str(token.text)\n",
    "                    entities_dict['duration'] = duration_hours+\" hours\"\n",
    "                    duration_entities['h'] = duration_hours\n",
    "                    duration_entities['m'] = 0\n",
    "                    entities_dict['duration'] = duration_entities\n",
    "\n",
    "                # print(\"Duration:\", duration, \"hours\")\n",
    "\n",
    "                elif next_token and (next_token.text == \"minutes\"):\n",
    "                    duration_minutes = str(token.text)\n",
    "                    duration_entities['h'] = duration_hours\n",
    "                    duration_entities['m'] = duration_minutes\n",
    "                    entities_dict['duration'] = duration_entities'''\n",
    "\n",
    "    if (type == \"Project\"):\n",
    "\n",
    "        extractName(query,entities_dict)\n",
    "        duration_hours=0\n",
    "        duration_minutes=0\n",
    "\n",
    "        pattern2 = r\"\\d{2}/\\d{2}/\\d{4}\\s\\d{2}:\\d{2}\"\n",
    "        matches = re.findall(pattern2, query)\n",
    "        entities_dict['due'] = matches[0]\n",
    "        \n",
    "        for token in doc:\n",
    "        # Check if the token is a number\n",
    "            if token.pos_ == \"NUM\":\n",
    "                    next_token = doc[token.i+1] if token.i+1 < len(doc) else None\n",
    "                    if next_token and (next_token.text == \"hours\"):\n",
    "                            duration_hours = str(token.text)\n",
    "                            #print(duration_hours)\n",
    "                            if token1 in doc:\n",
    "                                    for token in doc:\n",
    "                                            if next_token and (next_token.text == \"minutes\"):\n",
    "                                                    duration_minutes=str(token.text)\n",
    "                                            else:\n",
    "                                                duration_minutes=0\n",
    "                        \n",
    "\n",
    "                    \n",
    "\n",
    "                    if next_token and (next_token.text == \"minutes\"):\n",
    "                        duration_minutes=str(token.text)\n",
    "                        if token2 in doc:\n",
    "                                    for token in doc:\n",
    "                                            if next_token and (next_token.text == \"hours\"):\n",
    "                                                    duration_hours=str(token.text)\n",
    "                                            else:\n",
    "                                                duration_hours=0\n",
    "\n",
    "        duration_entities['h']=duration_hours\n",
    "        duration_entities['m']=duration_minutes\n",
    "        entities_dict['duration']=duration_entities   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if (type == \"Venture\"):\n",
    "\n",
    "        extractName(query,entities_dict)\n",
    "\n",
    "    return entities_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complaints-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
